{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9755189d",
   "metadata": {},
   "source": [
    "# MorphoXAI – Stage 1: Morphologic Spectrum Construction – Starter Notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83fe033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# project root = one level up from this notebook\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# Directory containing the metadata for all WSIs used to train the \n",
    "# full-data MIL model that will be explained in later steps.\n",
    "DATA_SPLIT_DIR = PROJECT_ROOT / \"Data_Split\" \n",
    "\n",
    "# Directory used to store all WSI feature bags (extracted by CONCH), one H5 file per slide.\n",
    "FEATURE_BAG_DIR = PROJECT_ROOT / \"feature_bags\"\n",
    "\n",
    "DATA_SPLIT_DIR.mkdir(exist_ok=True)\n",
    "FEATURE_BAG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Configuration file specifying label mappings, model settings, \n",
    "# and hyperparameters used throughout the MorphoXAI pipeline.\n",
    "CONFIG_PATH = PROJECT_ROOT / \"Morphologic_Spectrum_Construction\" / \"config.yaml\"\n",
    "\n",
    "DATA_SPLIT_DIR, FEATURE_BAG_DIR, CONFIG_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f16575",
   "metadata": {},
   "source": [
    "## Step 1 – Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643d1947",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_csv = DATA_SPLIT_DIR / \"output_svs_file_mapping.csv\" #the metadat csv file for all WSIs\n",
    "assert metadata_csv.exists(), f\"Metadata CSV not found: {metadata_csv}\"\n",
    "\n",
    "df_meta = pd.read_csv(metadata_csv)\n",
    "df_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d149ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Morphologic_Spectrum_Construction import run_patient_level_split\n",
    "\n",
    "mode = \"random\"\n",
    "split_num = 5 #number of patient permutations\n",
    "fold_num = 10 #number of folds\n",
    "train_ratio = 0.6\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "label_config = CONFIG_PATH\n",
    "\n",
    "output_paths = run_patient_level_split(\n",
    "    data_csv=metadata_csv,\n",
    "    output_dir=DATA_SPLIT_DIR,\n",
    "    label_config=label_config,\n",
    "    mode=mode,\n",
    "    split_num=split_num,\n",
    "    fold_num=fold_num,\n",
    "    train_ratio=train_ratio,\n",
    "    val_ratio=val_ratio,\n",
    "    test_ratio=test_ratio,\n",
    ")\n",
    "\n",
    "output_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6877a622",
   "metadata": {},
   "source": [
    "The resulting data splits are stored in DATA_SPLIT_DIR, named in the format `Datasplit_{s_index}_{n_folds}_fold_by_patient.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feeb4a0",
   "metadata": {},
   "source": [
    "## Step 2 – WSI → feature bags (CONCH embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fd8092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an example WSI from the metadata (here we use the first row; feel free to choose another one)\n",
    "row = df_meta.iloc[0]\n",
    "row\n",
    "# WSI_ROOT is the top-level directory where the original whole-slide images (WSIs) are stored\n",
    "WSI_ROOT = Path(\"/path/to/wholeslides\").resolve()\n",
    "print(WSI_ROOT)\n",
    "wsi_path = WSI_ROOT / \"xxx.svs\"\n",
    "wsi_path = wsi_path.resolve()\n",
    "print(wsi_path, wsi_path.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeb7a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_size = 360\n",
    "out_size = 224\n",
    "batch_size = 256\n",
    "num_workers = 8\n",
    "\n",
    "from Morphologic_Spectrum_Construction import preprocess_wsi_to_h5\n",
    "\n",
    "result = preprocess_wsi_to_h5(\n",
    "    input_slide=wsi_path,\n",
    "    output_dir=FEATURE_BAG_DIR,\n",
    "    tile_size_microns=360,\n",
    "    out_size=224,\n",
    "    batch_size=256,\n",
    "    workers=8,\n",
    "    hf_auth_token=\"\", #put your hugging face token here\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f96047",
   "metadata": {},
   "source": [
    "For each processed WSI, two types of files are produced in the `FEATURE_BAG_DIR`: `{Slide_ID}.h5`, `{Slide_ID}_features_QC.png`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6f5db8",
   "metadata": {},
   "source": [
    "## Step 3 – MIL training with CLAM \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9f2c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_index = 0          # which split index (patient permutation) from Data Preparation\n",
    "n_folds = 10         # must match the value used in Data Preparation\n",
    "\n",
    "manifest_name = f\"Datasplit_{s_index}_{n_folds}_fold_by_patient.csv\"\n",
    "manifest_path = DATA_SPLIT_DIR / manifest_name\n",
    "\n",
    "print(\"Manifest path:\", manifest_path)\n",
    "print(\"Exists:\", manifest_path.exists())\n",
    "\n",
    "feature_bag_dir = Path(\"/path/to/feature_bags\")\n",
    "print(\"Feature bag dir:\", feature_bag_dir, \"Exists:\", feature_bag_dir.exists())\n",
    "\n",
    "#Directory where training checkpoints will be saved\n",
    "out_checkpoint_dir = PROJECT_ROOT/ \"runs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38680e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Morphologic_Spectrum_Construction import train_single_round\n",
    "round_idx = 0\n",
    "\n",
    "train_single_round(\n",
    "    manifest=manifest_path,\n",
    "    feature_bag_dir=feature_bag_dir,\n",
    "    out_checkpoint_dir=out_checkpoint_dir,\n",
    "    round_idx=0,                   \n",
    "    workers=4,\n",
    "    full_training_index=None,      \n",
    "    config_path=CONFIG_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8606aff",
   "metadata": {},
   "source": [
    "Checkpoints are saved under `./runs/` in subfolders named according to the data split and the round index, e.g.:\n",
    "\n",
    "  ```\n",
    "  runs/Datasplit_0_10_fold_by_patient_random/round_0_small_random_hp1/\n",
    "  ```\n",
    "This example corresponds to the best model obtained from the first shuffle (`Datasplit_0_10_fold_by_patient_random`) during round 0 of cross-validation.\n",
    "\n",
    "Each subfolder contains the **best model checkpoint** (selected based on validation performance) in PyTorch format:\n",
    "  ```\n",
    "  {round}_{epoch}_checkpoint.pt\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f31627f",
   "metadata": {},
   "source": [
    "## Step 4 – MIL testing (evaluation on held-out test sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b3b819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manifest from Step 1 (same as training)\n",
    "s_index = 0\n",
    "n_folds = 10\n",
    "manifest_name = f\"Datasplit_{s_index}_{n_folds}_fold_by_patient.csv\"\n",
    "manifest_path = DATA_SPLIT_DIR / manifest_name\n",
    "print(\"Manifest:\", manifest_path, \"Exists:\", manifest_path.exists())\n",
    "\n",
    "# feature bags (same directory used for training)\n",
    "feature_bag_dir = Path(\"/path/to/feature_bags\")\n",
    "print(\"Feature bag dir:\", feature_bag_dir, \"Exists:\", feature_bag_dir.exists())\n",
    "\n",
    "# checkpoints directory = training outputs (./runs)\n",
    "checkpoint_dir = PROJECT_ROOT/ \"runs\"\n",
    "print(\"Checkpoint dir:\", checkpoint_dir, \"Exists:\", checkpoint_dir.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3c086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Morphologic_Spectrum_Construction import evaluate_all_rounds\n",
    "\n",
    "evaluate_all_rounds(\n",
    "    manifest=manifest_path,\n",
    "    feature_bag_dir=feature_bag_dir,\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    round_num=1, # number of CV rounds to evaluate\n",
    "    workers=4,\n",
    "    config_path=CONFIG_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26143fba",
   "metadata": {},
   "source": [
    "All outputs are organized under `./test_result/{dataset_name}/` for each dataset, where `{dataset_name}` corresponds to the CSV manifest used (e.g. `Datasplit_0_10_fold_by_patient`). **ROC curve figures** are stored under `./ROC_Curve/{dataset_name}/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f9d524",
   "metadata": {},
   "source": [
    "## Step 5 – Sample Stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4578de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Morphologic_Spectrum_Construction import run_sample_stratification\n",
    "\n",
    "run_sample_stratification(\n",
    "    test_result_root=PROJECT_ROOT/ \"test_result\",\n",
    "    metadata_csv=DATA_SPLIT_DIR / \"output_svs_file_mapping.csv\",\n",
    "    out_dir=PROJECT_ROOT/ \"test_result\",\n",
    "    config_path=CONFIG_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae60bb41",
   "metadata": {},
   "source": [
    "The output `sample_stratification_result.csv` is saved under `out_dir`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4534f85e",
   "metadata": {},
   "source": [
    "## Step 6 – High-contribution Patches Exatraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066228f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing all CONCH feature bags (*.h5)\n",
    "feature_bag_dir = Path(\"/path/to/feature_bags\")\n",
    "print(\"Feature bag dir:\", feature_bag_dir, \"Exists:\", feature_bag_dir.exists())\n",
    "\n",
    "#Sample stratification CSV (output of Step 5)\n",
    "sample_stratf_file=PROJECT_ROOT / \"test_result\" / \"sample_stratification_result.csv\"\n",
    "\n",
    "#Directory where final high-contribution patches will be saved\n",
    "output_dir=PROJECT_ROOT / \"test_result\" / \"high_contribution_patches\"\n",
    "\n",
    "#Directory where MIL model checkpoints are stored (output of Step 3)\n",
    "runs_root=PROJECT_ROOT / \"runs\"\n",
    "\n",
    "#Directory containing all Datasplit_*.csv files (output of Step 1)\n",
    "datasplit_root = PROJECT_ROOT / \"Data_Split\"\n",
    "datasplit_root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f708b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Morphologic_Spectrum_Construction import extract_high_contribution_patches\n",
    "coords_csv = extract_high_contribution_patches(\n",
    "    feature_bag_dir=feature_bag_dir,\n",
    "    sample_stratf_file=sample_stratf_file,\n",
    "    output_dir=output_dir,\n",
    "    subtype=\"Clear\",  # Short class name; must match LABEL_MAP_SHORT in config.yaml\n",
    "    stability=\"Consistently_Correct\",\n",
    "    topk_threshold=0.9,\n",
    "    config_path=CONFIG_PATH,\n",
    "    runs_root=runs_root,\n",
    "    datasplit_root=datasplit_root,\n",
    "    n_splits=5,\n",
    "    n_folds=10,\n",
    ")\n",
    "coords_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fe452b",
   "metadata": {},
   "source": [
    "The final results are written to a single CSV file named `{subtype}_{stability}.csv` in the directory specified by `output_dir`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bddb2e",
   "metadata": {},
   "source": [
    "## Step 7 – High-contribution Patches Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bbcfe8",
   "metadata": {},
   "source": [
    "The script supports two modes:\n",
    "1. **`evaluate_clusters`**  \n",
    "   Run consensus clustering on a micro-clustered subset of embeddings to evaluate\n",
    "   a list of candidate `k` values. This helps us decide a reasonable number of clusters.\n",
    "2. **`export`**  \n",
    "   Given a chosen `k` (number of clusters), perform hierarchical clustering on\n",
    "   all patches, assign each patch an `hc_label`, and export."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fb5bb7",
   "metadata": {},
   "source": [
    "### Mode 1 — `evaluate_clusters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee0d994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Metadata CSV (slide_id → patient / tissue mapping)\n",
    "metadata_csv = PROJECT_ROOT / \"Data_Split\" / \"output_svs_file_mapping.csv\"\n",
    "print(\"metadata_csv:\", metadata_csv, \"| Exists:\", metadata_csv.exists())\n",
    "\n",
    "# 2) High-contribution patch CSV to cluster\n",
    "#    This is the output from high_contribution_patches_extraction (Step 6),\n",
    "#    typically named {subtype}_{stability}_all.csv\n",
    "high_contri_csv = PROJECT_ROOT / \"test_result\" / \"high_contribution_patches\" / \"Clear_Consistently_Correct_all.csv\"\n",
    "print(\"high_contri_csv:\", high_contri_csv, \"| Exists:\", high_contri_csv.exists())\n",
    "\n",
    "# 3) Disease / subtype label (short name, must match the 'label' column in high_contri_csv)\n",
    "disease = \"Clear\"   # e.g., \"Clear\", \"Endo\", ...\n",
    "\n",
    "# 4) Output directory for clustering results\n",
    "cluster_out_dir = PROJECT_ROOT / \"high_contribution_cluster\"\n",
    "cluster_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(\"cluster_out_dir:\", cluster_out_dir, \"| Exists:\", cluster_out_dir.exists())\n",
    "\n",
    "# 5) Feature bag directory (only required in 'export' mode)\n",
    "feature_bag_dir = Path(\"/path/to/feature_bags\")\n",
    "print(\"Feature bag dir:\", feature_bag_dir, \"Exists:\", feature_bag_dir.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d6e08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Morphologic_Spectrum_Construction import cluster_high_contribution_patches\n",
    "\n",
    "# Candidate k values to evaluate\n",
    "cluster_list = [3, 4, 5, 6, 7, 8]\n",
    "\n",
    "# Consensus clustering hyperparameters\n",
    "reps = 30      # number of subsampling runs\n",
    "p_item = 0.8   # subsampling fraction per run\n",
    "n_micro = 1000 # number of FAISS micro-centroids\n",
    "\n",
    "cluster_high_contribution_patches(\n",
    "    metadata_csv=metadata_csv,\n",
    "    high_contri_csv=high_contri_csv,\n",
    "    disease=disease,\n",
    "    mode=\"evaluate_clusters\",\n",
    "    out_dir=cluster_out_dir,\n",
    "    feature_bag_dir=None,     # Not used in this mode\n",
    "    cluster_list=cluster_list,\n",
    "    reps=reps,\n",
    "    p_item=p_item,\n",
    "    n_micro=n_micro,\n",
    "    n_clusters=None,          # Ignored in this mode\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f588fb",
   "metadata": {},
   "source": [
    "Two Outputs would be saved in `out_dir`:\n",
    "- `consensus_k_selection.csv` — table of `k` vs. cophenetic correlation (higher is better).\n",
    "- `consensus_heatmap_k{k}.png` — consensus matrices visualizing clustering stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dc9443",
   "metadata": {},
   "source": [
    "### Mode 2 — `export`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdcdcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we decided that k = 5 is a good choice from evaluate_clusters\n",
    "chosen_k = 5\n",
    "\n",
    "from Morphologic_Spectrum_Construction import cluster_high_contribution_patches\n",
    "\n",
    "cluster_high_contribution_patches(\n",
    "    metadata_csv=metadata_csv,\n",
    "    high_contri_csv=high_contri_csv,\n",
    "    disease=disease,\n",
    "    mode=\"export\",\n",
    "    out_dir=cluster_out_dir,\n",
    "    feature_bag_dir=feature_bag_dir,  # Required in export mode\n",
    "    cluster_list=None,                # Ignored here\n",
    "    reps=reps,\n",
    "    p_item=p_item,\n",
    "    n_micro=n_micro,\n",
    "    n_clusters=chosen_k,              # <-- key parameter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c85f18d",
   "metadata": {},
   "source": [
    "The results are stored in `out_dir\\slide_results`. For each slide, a file named `<slide_id>_hc_coords.csv` is generated, containing patch-level coordinates assigned to each hierarchical cluster (`hc_label`). These files can be directly imported into the **MorphoCA** QuPath plugin for visualization. For detailed instructions, please refer to `./MorphoSpectrum-MIL/plugins/MorphoCA Manual.pdf`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed76912",
   "metadata": {},
   "source": [
    "## Step 8 – Export Morphologic Spectrum Stats\n",
    "This step consolidates all information needed to build the morphologic spectrum, and saves it as a single .pkl file. The exported spectrum stats will be used directly in MorphoXAI Stage 2 for spectrum-based explanations on the independent test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b01afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Morphologic_Spectrum_Construction.export_spectrum import export_spectrum_stats\n",
    "import yaml\n",
    "\n",
    "with open(CONFIG_PATH) as f:\n",
    "    CFG = yaml.safe_load(f)\n",
    "\n",
    "algo_cfg = CFG[\"algorithm\"]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Step 8 inputs are DIRECTLY linked to outputs from Step 6 and Step 7.\n",
    "#\n",
    "# Conventions:\n",
    "# - Step 6 (high-contribution patch extraction) outputs:\n",
    "#     {hc_patch_dir}/{group}_{stability}_all.csv\n",
    "# - Step 7 (clustering export) outputs:\n",
    "#     {cluster_out_dir}/{group}/slide_results/\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# Roots for Step 6 and Step 7 outputs\n",
    "hc_patch_dir = PROJECT_ROOT / \"test_result\" / \"high_contribution_patches\"    # Step 6 outputs\n",
    "cluster_out_dir = PROJECT_ROOT / \"high_contribution_cluster\"                 # Step 7 outputs\n",
    "\n",
    "# -----------------------------\n",
    "# Core patterns (Consistently_Correct slides)\n",
    "# -----------------------------\n",
    "# Step 6 outputs: patch tables with embeddings\n",
    "core_embeds = {\n",
    "    \"Clear\":  hc_patch_dir / \"Clear_Consistently_Correct_all.csv\",\n",
    "    \"Endo\":   hc_patch_dir / \"Endo_Consistently_Correct_all.csv\",\n",
    "    \"High\":   hc_patch_dir / \"High_Consistently_Correct_all.csv\",\n",
    "    \"Border\": hc_patch_dir / \"Border_Consistently_Correct_all.csv\",\n",
    "}\n",
    "\n",
    "# Step 7 outputs: QuPath-ready patch coordinate exports (per slide)\n",
    "core_clusters = {\n",
    "    \"Clear\":  cluster_out_dir / \"Clear\"  / \"slide_results\",\n",
    "    \"Endo\":   cluster_out_dir / \"Endo\"   / \"slide_results\",\n",
    "    \"High\":   cluster_out_dir / \"High\"   / \"slide_results\",\n",
    "    \"Border\": cluster_out_dir / \"Border\" / \"slide_results\",\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Transitional patterns (Highly_Variable slides)\n",
    "# -----------------------------\n",
    "# Step 6 outputs: patch tables with embeddings\n",
    "transition_embeds = {\n",
    "    \"Endo_to_High\": hc_patch_dir / \"Endo_to_High_Highly_Variable_all.csv\",\n",
    "    \"High_to_Endo\": hc_patch_dir / \"High_to_Endo_Highly_Variable_all.csv\",\n",
    "}\n",
    "\n",
    "# Step 7 outputs: QuPath-ready patch coordinate exports (per slide)\n",
    "transition_clusters = {\n",
    "    \"Endo_to_High\": cluster_out_dir / \"Endo_to_High\" / \"slide_results\",\n",
    "    \"High_to_Endo\": cluster_out_dir / \"High_to_Endo\" / \"slide_results\",\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Step 8 output\n",
    "# -----------------------------\n",
    "SPECTRUM_STATS_PATH = PROJECT_ROOT / \"morphologic_spectrum_stats.pkl\"\n",
    "\n",
    "export_spectrum_stats(\n",
    "    core_embeds=core_embeds,\n",
    "    core_clusters=core_clusters,\n",
    "    transition_embeds=transition_embeds,\n",
    "    transition_clusters=transition_clusters,\n",
    "    algorithm_cfg=algo_cfg,\n",
    "    output_path=SPECTRUM_STATS_PATH,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CONCH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
